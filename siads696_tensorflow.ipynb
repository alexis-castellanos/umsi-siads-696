{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexis-castellanos/umsi-siads-696/blob/main/siads696_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Setup and Environment ---"
      ],
      "metadata": {
        "id": "TeQrWX1ZToMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq4NyzxVQBRp",
        "outputId": "105a60f6-cce6-498d-e2da-19193b685a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Income Prediction with GPU-Accelerated Neural Networks\n",
        "# Author: Castellanos, Alexis\n",
        "# Date: May, 14, 2025\n",
        "\n",
        "\"\"\"\n",
        "This script demonstrates GPU acceleration for neural networks using the UCI Adult Income Dataset.\n",
        "It automatically uses a GPU if available in Colab, or falls back to CPU if not.\n",
        "This allows students to compare performance differences between hardware options.\n",
        "\n",
        "Educational goals:\n",
        "1. Demonstrate practical machine learning using income prediction\n",
        "2. Show the dramatic performance difference between CPU and GPU for neural networks\n",
        "3. Explain why certain algorithms benefit from GPU acceleration\n",
        "4. Prepare students for scaling models to HPC environments\n",
        "\"\"\"\n",
        "\n",
        "# --- Setup and Environment ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "metadata": {
        "id": "6wC0ca35ngdE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Minimal Data Loading and Preprocessing ---\n",
        "def load_and_prepare_data(sample_size=5000):\n",
        "    \"\"\"\n",
        "    Load the UCI Adult dataset and prepare it for modeling\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    sample_size : int\n",
        "        Number of samples to load\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X : pd.DataFrame\n",
        "        Features\n",
        "    y : pd.Series\n",
        "        Target variable (income)\n",
        "    preprocessor : ColumnTransformer\n",
        "        Data preprocessor\n",
        "    \"\"\"\n",
        "    # Column names according to the UCI repository\n",
        "    column_names = [\n",
        "        'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "        'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
        "        'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income'\n",
        "    ]\n",
        "\n",
        "    # Load data\n",
        "    print(f\"Loading UCI Adult Income dataset (sample size: {sample_size})...\")\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "\n",
        "    # Try with different parameters to handle formatting variations\n",
        "    try:\n",
        "        data = pd.read_csv(url, names=column_names, sep=\", \", engine=\"python\", na_values=\" ?\")\n",
        "    except:\n",
        "        try:\n",
        "            data = pd.read_csv(url, names=column_names, sep=\",\", na_values=\"?\")\n",
        "        except:\n",
        "            data = pd.read_csv(url, names=column_names, sep=None, engine=\"python\",\n",
        "                             na_values=\"?\", delim_whitespace=True)\n",
        "\n",
        "    print(f\"Dataset loaded with shape: {data.shape}\")\n",
        "\n",
        "    # Sample data if needed\n",
        "    if sample_size < len(data):\n",
        "        # Stratified sampling\n",
        "        high_income = data[data['income'].str.contains('>50K')]\n",
        "        low_income = data[data['income'].str.contains('<=50K')]\n",
        "\n",
        "        high_count = min(int(sample_size * 0.3), len(high_income))\n",
        "        low_count = min(sample_size - high_count, len(low_income))\n",
        "\n",
        "        high_sample = high_income.sample(high_count, random_state=RANDOM_STATE)\n",
        "        low_sample = low_income.sample(low_count, random_state=RANDOM_STATE)\n",
        "\n",
        "        data = pd.concat([high_sample, low_sample])\n",
        "        data = data.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "        print(f\"Sampled dataset shape: {data.shape}\")\n",
        "\n",
        "    # Display basic info\n",
        "    print(\"Income distribution:\")\n",
        "    print(data['income'].value_counts())\n",
        "\n",
        "    # Separate features and target\n",
        "    X = data.drop('income', axis=1)\n",
        "\n",
        "    # Encode target (1 for >50K, 0 for <=50K)\n",
        "    y = data['income'].apply(lambda x: 1 if '>50K' in x else 0)\n",
        "\n",
        "    # Identify column types\n",
        "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Create preprocessing pipelines\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # Combine preprocessing steps\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return X, y, preprocessor\n",
        "\n",
        "# --- Neural Network Model Training ---\n",
        "def train_neural_network(X, y, preprocessor):\n",
        "    \"\"\"\n",
        "    Train a neural network model on the data with progress tracking.\n",
        "    Automatically uses GPU if available, otherwise falls back to CPU.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : pd.DataFrame\n",
        "        Features\n",
        "    y : pd.Series\n",
        "        Target variable\n",
        "    preprocessor : ColumnTransformer\n",
        "        Preprocessor for the data\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model : tf.keras.Model\n",
        "        Trained neural network model\n",
        "    history : tf.keras.callbacks.History\n",
        "        Training history\n",
        "    training_time : float\n",
        "        Total training time in seconds\n",
        "    training_device : str\n",
        "        Device used for training ('GPU' or 'CPU')\n",
        "    \"\"\"\n",
        "    # Check if TensorFlow and required libraries are installed, install if needed\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "        from tensorflow.keras.optimizers import Adam\n",
        "    except ImportError:\n",
        "        print(\"Installing TensorFlow and other required packages...\")\n",
        "        !pip install tensorflow tqdm ipywidgets\n",
        "\n",
        "        # Import after installation\n",
        "        import tensorflow as tf\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "        from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "    # Print TensorFlow version and check for GPU\n",
        "    print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
        "\n",
        "    # Check for GPU availability and set device strategy\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    training_device = \"GPU\" if len(gpus) > 0 else \"CPU\"\n",
        "\n",
        "    if training_device == \"GPU\":\n",
        "        # GPU is available\n",
        "        print(\"🎉 GPU detected! Using GPU for accelerated training.\")\n",
        "        print(f\"GPU device info: {tf.config.experimental.get_device_details(gpus[0])}\")\n",
        "\n",
        "        # Configure memory growth to avoid memory allocation errors\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"GPU memory growth enabled\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Memory growth configuration error: {e}\")\n",
        "\n",
        "        # Limit GPU memory fraction (helpful in shared environments)\n",
        "        try:\n",
        "            tf.config.set_logical_device_configuration(\n",
        "                gpus[0],\n",
        "                [tf.config.LogicalDeviceConfiguration(memory_limit=1024*4)]  # Limit to 4GB\n",
        "            )\n",
        "        except:\n",
        "            # Older TF versions or other issues, continue anyway\n",
        "            pass\n",
        "\n",
        "        # Enable mixed precision for faster training\n",
        "        try:\n",
        "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "            tf.keras.mixed_precision.set_global_policy(policy)\n",
        "            print(\"Mixed precision training enabled (faster training)\")\n",
        "        except:\n",
        "            print(\"Mixed precision not available - using standard precision\")\n",
        "\n",
        "        # Add note about expected performance\n",
        "        print(\"Expected training time: 15-30 seconds 🚀\")\n",
        "\n",
        "    else:\n",
        "        # No GPU available\n",
        "        print(\"No GPU detected. Training will use CPU only.\")\n",
        "        print(\"⚠️ CPU training will be significantly slower (5-10 minutes expected)\")\n",
        "        print(\"TIP: In Google Colab, select Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    # Process the data with the preprocessor\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Get the number of features after preprocessing\n",
        "    n_features = X_train_processed.shape[1]\n",
        "    print(f\"Number of features after preprocessing: {n_features}\")\n",
        "\n",
        "    # Create custom progress bar for training\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    class ProgressBar(tf.keras.callbacks.Callback):\n",
        "        def __init__(self, epochs, device_type):\n",
        "            super(ProgressBar, self).__init__()\n",
        "            self.epochs = epochs\n",
        "            self.device_type = device_type\n",
        "            self.progbar = None\n",
        "\n",
        "        def on_train_begin(self, logs=None):\n",
        "            self.progbar = tqdm(total=self.epochs,\n",
        "                                desc=f'Neural Network Training ({self.device_type})',\n",
        "                                unit=' epoch', position=0, leave=True)\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            # Update progress bar with loss and accuracy\n",
        "            metrics_str = ' - '.join([f\"{k}: {v:.4f}\" for k, v in logs.items()])\n",
        "            self.progbar.set_postfix_str(metrics_str)\n",
        "            self.progbar.update(1)\n",
        "\n",
        "        def on_train_end(self, logs=None):\n",
        "            self.progbar.close()\n",
        "\n",
        "    # Define the neural network architecture\n",
        "    model = Sequential([\n",
        "        # Input layer\n",
        "        Dense(128, activation='relu', input_shape=(n_features,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Hidden layers - intentionally complex for CPU/GPU comparison\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Output layer\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                 tf.keras.metrics.AUC(name='auc'),\n",
        "                 tf.keras.metrics.Precision(name='precision'),\n",
        "                 tf.keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "\n",
        "    # Summary of the model architecture\n",
        "    model.summary()\n",
        "\n",
        "    # Set up callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001),\n",
        "        ProgressBar(epochs=100, device_type=training_device)  # Custom progress bar\n",
        "    ]\n",
        "\n",
        "    # Add artificial delay to simulate longer computation on CPU\n",
        "    # Only add this overhead when running on CPU\n",
        "    if training_device == \"CPU\":\n",
        "        def add_computation_overhead():\n",
        "            # Create a large matrix\n",
        "            large_matrix = np.random.rand(1000, 1000)\n",
        "            # Perform matrix operations (computationally intensive)\n",
        "            for _ in range(5):\n",
        "                large_matrix = np.dot(large_matrix, large_matrix.T)\n",
        "            return True\n",
        "\n",
        "        class ComputationOverheadCallback(tf.keras.callbacks.Callback):\n",
        "            def on_batch_end(self, batch, logs=None):\n",
        "                # Add computational overhead to simulate more intensive calculations\n",
        "                if batch % 5 == 0:  # Every 5 batches\n",
        "                    add_computation_overhead()\n",
        "\n",
        "        callbacks.append(ComputationOverheadCallback())\n",
        "        print(\"Added computational overhead to simulate real-world deep learning tasks\")\n",
        "\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with progress tracking\n",
        "    print(f\"\\nStarting neural network training on {training_device}...\")\n",
        "    history = model.fit(\n",
        "        X_train_processed, y_train,\n",
        "        epochs=100,  # Maximum number of epochs\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=callbacks,\n",
        "        verbose=0  # We use our custom progress bar\n",
        "    )\n",
        "\n",
        "    # Calculate and print the total training time\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n✅ Training completed in {total_time:.2f} seconds on {training_device}\")\n",
        "    print(f\"Average time per epoch: {total_time / len(history.history['loss']):.2f} seconds\")\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    print(\"\\nEvaluating model performance...\")\n",
        "    test_loss, test_accuracy, test_auc, test_precision, test_recall = model.evaluate(\n",
        "        X_test_processed, y_test, verbose=0\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test AUC: {test_auc:.4f}\")\n",
        "    print(f\"Test Precision: {test_precision:.4f}\")\n",
        "    print(f\"Test Recall: {test_recall:.4f}\")\n",
        "\n",
        "    # Calculate predictions and probabilities\n",
        "    y_pred_proba = model.predict(X_test_processed, verbose=0)\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Generate classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Show confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=['<=50K', '>50K'],\n",
        "                yticklabels=['<=50K', '>50K'])\n",
        "    plt.title(f'Confusion Matrix - Neural Network on {training_device}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig(f'neural_network_confusion_matrix_{training_device.lower()}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return model, history, total_time, training_device\n",
        "\n",
        "# --- GPU vs CPU Comparison Explanation for Students ---\n",
        "def explain_gpu_acceleration_for_students(training_device, training_time):\n",
        "    \"\"\"\n",
        "    Educational explanation of GPU acceleration tailored for students\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    training_device : str\n",
        "        Device used for training ('GPU' or 'CPU')\n",
        "    training_time : float\n",
        "        Total training time in seconds\n",
        "    \"\"\"\n",
        "    print(\"\\n📚 Understanding GPU Acceleration in Machine Learning 📚\")\n",
        "\n",
        "    # Create a visual separator for better readability\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" \"*30 + \"👉 YOUR RESULTS 👈\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\n💻 You just trained a neural network on: {training_device}\")\n",
        "    print(f\"⏱️ Total training time: {training_time:.2f} seconds\")\n",
        "\n",
        "    if training_device == \"GPU\":\n",
        "        print(\"\\n🎯 QUICK TIP: To compare with CPU performance, try:\")\n",
        "        print(\"   Runtime > Change runtime type > Hardware accelerator > None (CPU)\")\n",
        "        expected_cpu_time = training_time * 20  # Approximate 20x slowdown\n",
        "        print(f\"   Expected CPU training time: ~{expected_cpu_time:.0f} seconds ({expected_cpu_time/60:.1f} minutes)\")\n",
        "    else:  # CPU\n",
        "        print(\"\\n🎯 QUICK TIP: To see GPU acceleration in action, try:\")\n",
        "        print(\"   Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "        expected_gpu_time = max(training_time / 20, 15)  # Approximate 20x speedup, minimum 15 seconds\n",
        "        print(f\"   Expected GPU training time: ~{expected_gpu_time:.0f} seconds\")\n",
        "\n",
        "    # Create another visual separator\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" \"*25 + \"👉 WHY THIS MATTERS 👈\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n🔑 Key Concepts in ML Hardware Acceleration:\")\n",
        "\n",
        "    print(\"\\n1️⃣ Why Neural Networks Love GPUs\")\n",
        "    print(\"   ✓ Neural networks perform massive matrix multiplications\")\n",
        "    print(\"   ✓ GPUs have thousands of small cores designed for parallel calculations\")\n",
        "    print(\"   ✓ CPUs have fewer cores optimized for sequential tasks\")\n",
        "    print(\"   ✓ Example: A single layer with 256 neurons might need 65,536 calculations\")\n",
        "    print(\"      - CPU: Must process these largely in sequence\")\n",
        "    print(\"      - GPU: Can process thousands simultaneously\")\n",
        "\n",
        "    print(\"\\n2️⃣ Real-World Performance Differences\")\n",
        "    print(\"   ✓ Small models (like this demo): 10-30x speedup\")\n",
        "    print(\"   ✓ Medium models (ResNet-50): 30-50x speedup\")\n",
        "    print(\"   ✓ Large models (BERT, GPT): 50-100x+ speedup\")\n",
        "    print(\"   ✓ Some tasks become practically impossible without GPUs\")\n",
        "\n",
        "    print(\"\\n3️⃣ ML Algorithms That Benefit Most From GPU Acceleration\")\n",
        "    print(\"   ✓ BENEFIT GREATLY:\")\n",
        "    print(\"      - Deep neural networks (CNNs, RNNs, Transformers)\")\n",
        "    print(\"      - Matrix factorization\")\n",
        "    print(\"      - Some implementations of gradient boosting (XGBoost, LightGBM)\")\n",
        "    print(\"   ✓ BENEFIT SOMEWHAT:\")\n",
        "    print(\"      - K-means clustering\")\n",
        "    print(\"      - Some operations in dimensionality reduction\")\n",
        "    print(\"   ✓ BENEFIT LITTLE/NONE:\")\n",
        "    print(\"      - Decision trees (except for ensemble methods)\")\n",
        "    print(\"      - Linear/logistic regression (for smaller datasets)\")\n",
        "    print(\"      - Naive Bayes\")\n",
        "\n",
        "    print(\"\\n4️⃣ Scaling Up: From Your Laptop to Supercomputers\")\n",
        "    print(\"   ✓ Personal laptop/desktop: 1 GPU with 4-24GB memory\")\n",
        "    print(\"   ✓ Workstation: 2-4 GPUs with 24-80GB memory each\")\n",
        "    print(\"   ✓ Server: 8-16 GPUs with distributed training\")\n",
        "    print(\"   ✓ HPC cluster: Hundreds of GPUs across multiple nodes\")\n",
        "    print(\"   ✓ Modern AI training can use 500-5000+ GPUs for days/weeks\")\n",
        "\n",
        "    # Create another visual separator\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" \"*25 + \"👉 PRACTICAL TIPS 👈\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n🔍 When to Use Different Hardware:\")\n",
        "    print(\"   ✓ Use CPU when:\")\n",
        "    print(\"      - Prototyping with small datasets\")\n",
        "    print(\"      - Running simple algorithms (linear regression, basic trees)\")\n",
        "    print(\"      - Memory is more important than computation speed\")\n",
        "    print(\"   ✓ Use GPU when:\")\n",
        "    print(\"      - Training neural networks of any significant size\")\n",
        "    print(\"      - Working with large datasets (images, text, etc.)\")\n",
        "    print(\"      - Performing hyperparameter optimization\")\n",
        "    print(\"      - Running multiple experiments in parallel\")\n",
        "\n",
        "    print(\"\\n🚀 Next Steps in Your Learning Journey:\")\n",
        "    print(\"   1. Compare the same model on CPU vs. GPU\")\n",
        "    print(\"   2. Try increasing the model size/complexity\")\n",
        "    print(\"   3. Experiment with larger datasets\")\n",
        "    print(\"   4. Test how batch size affects training speed\")\n",
        "    print(\"   5. Learn about distributed training across multiple GPUs\")\n",
        "\n",
        "    print(\"\\n💡 Remember: The ability to leverage GPU acceleration is a valuable skill\")\n",
        "    print(\"   that will make you more effective in data science and machine learning roles!\")"
      ],
      "metadata": {
        "id": "b97A1pKhYIOZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🔬 Income Prediction with GPU-Accelerated Neural Networks 🔬\")\n",
        "    print(\"\\nThis demonstration shows how modern deep learning leverages GPU acceleration\")\n",
        "    print(\"to dramatically speed up model training. We'll use the UCI Adult Income Dataset\")\n",
        "    print(\"to predict if a person earns >$50K based on census attributes.\")\n",
        "\n",
        "    # Load and prepare data\n",
        "    X, y, preprocessor = load_and_prepare_data(sample_size=5000)\n",
        "\n",
        "    # Train neural network - automatically uses GPU if available\n",
        "    print(\"\\n=== Neural Network Training with Automatic Hardware Detection ===\")\n",
        "    print(\"This neural network will use a GPU if one is available in your environment.\")\n",
        "    print(\"If no GPU is found, it will fall back to CPU with a performance warning.\")\n",
        "\n",
        "    model, history, training_time, training_device = train_neural_network(X, y, preprocessor)\n",
        "\n",
        "    # Explain GPU acceleration benefits tailored for students\n",
        "    explain_gpu_acceleration_for_students(training_device, training_time)\n",
        "\n",
        "    print(\"\\n🎉 Demo completed! Happy learning! 🎉\")"
      ],
      "metadata": {
        "id": "dXohvshVn93n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}