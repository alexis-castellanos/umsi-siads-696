{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw6+LHiuxeN8Tv1bDkeKuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexis-castellanos/umsi-siads-696/blob/main/siads696_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEZfbnd4t-p9"
      },
      "outputs": [],
      "source": [
        "##########################################\n",
        "# PYTORCH IMPLEMENTATION\n",
        "##########################################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
        "\n",
        "# Verify environment\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Define data loading function\n",
        "def load_and_prepare_data(sample_size=5000):\n",
        "    \"\"\"Load and preprocess UCI Adult income dataset\"\"\"\n",
        "    # Column names for the dataset\n",
        "    column_names = [\n",
        "        'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "        'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
        "        'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income'\n",
        "    ]\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading UCI Adult Income dataset...\")\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "\n",
        "    # Try different loading parameters\n",
        "    try:\n",
        "        data = pd.read_csv(url, names=column_names, sep=\", \", engine=\"python\", na_values=\" ?\")\n",
        "    except:\n",
        "        try:\n",
        "            data = pd.read_csv(url, names=column_names, sep=\",\", na_values=\"?\")\n",
        "        except:\n",
        "            data = pd.read_csv(url, names=column_names, sep=None, engine=\"python\",\n",
        "                             na_values=\"?\", delim_whitespace=True)\n",
        "\n",
        "    # Sample data if needed\n",
        "    if sample_size < len(data):\n",
        "        # Stratified sampling\n",
        "        high_income = data[data['income'].str.contains('>50K')]\n",
        "        low_income = data[data['income'].str.contains('<=50K')]\n",
        "\n",
        "        high_count = min(int(sample_size * 0.3), len(high_income))\n",
        "        low_count = min(sample_size - high_count, len(low_income))\n",
        "\n",
        "        high_sample = high_income.sample(high_count, random_state=RANDOM_STATE)\n",
        "        low_sample = low_income.sample(low_count, random_state=RANDOM_STATE)\n",
        "\n",
        "        data = pd.concat([high_sample, low_sample])\n",
        "        data = data.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Dataset shape: {data.shape}\")\n",
        "    print(f\"Income distribution: {data['income'].value_counts().to_dict()}\")\n",
        "\n",
        "    # Separate features and target\n",
        "    X = data.drop('income', axis=1)\n",
        "    y = data['income'].apply(lambda x: 1 if '>50K' in x else 0)\n",
        "\n",
        "    # Identify column types\n",
        "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Create preprocessing pipelines\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # Combine preprocessing steps\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return X, y, preprocessor\n",
        "\n",
        "# Define PyTorch model\n",
        "class IncomeClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(IncomeClassifier, self).__init__()\n",
        "\n",
        "        # Input layer\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Hidden layers\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.output(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Function to train model\n",
        "def train_model(X, y, preprocessor):\n",
        "    \"\"\"Train PyTorch model with GPU acceleration if available\"\"\"\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    # Process features\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Get feature dimension\n",
        "    input_dim = X_train_processed.shape[1]\n",
        "    print(f\"Input dimension after preprocessing: {input_dim}\")\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train_processed)\n",
        "    y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1)\n",
        "    X_test_tensor = torch.FloatTensor(X_test_processed)\n",
        "    y_test_tensor = torch.FloatTensor(y_test.values).view(-1, 1)\n",
        "\n",
        "    # Create validation set\n",
        "    val_size = int(len(X_train_tensor) * 0.2)\n",
        "    X_val_tensor = X_train_tensor[-val_size:]\n",
        "    y_val_tensor = y_train_tensor[-val_size:]\n",
        "    X_train_tensor = X_train_tensor[:-val_size]\n",
        "    y_train_tensor = y_train_tensor[:-val_size]\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = IncomeClassifier(input_dim=input_dim)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model summary\n",
        "    print(model)\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, min_lr=0.00001, verbose=True\n",
        "    )\n",
        "\n",
        "    # Set training parameters\n",
        "    num_epochs = 100\n",
        "    patience = 10\n",
        "    counter = 0\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    # Metrics tracking\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # Start timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for inputs, labels in train_pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Stats\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100 * correct/total:.2f}%'\n",
        "            })\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = 100 * correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Print stats\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Training time\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Training completed in {total_time:.2f} seconds\")\n",
        "\n",
        "    # Load best model\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_probs.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    # Convert to numpy\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_probs = np.array(all_probs).flatten()\n",
        "    all_labels = np.array(all_labels).flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=['<=50K', '>50K'],\n",
        "                yticklabels=['<=50K', '>50K'])\n",
        "    plt.title('PyTorch Neural Network Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig('pytorch_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), 'income_classifier.pt')\n",
        "    print(\"Model saved to 'income_classifier.pt'\")\n",
        "\n",
        "    return model, {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs,\n",
        "        'accuracy': accuracy,\n",
        "        'training_time': total_time\n",
        "    }\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    X, y, preprocessor = load_and_prepare_data(sample_size=5000)\n",
        "\n",
        "    # Train model\n",
        "    model, metrics = train_model(X, y, preprocessor)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(metrics['train_losses'], label='Train')\n",
        "    plt.plot(metrics['val_losses'], label='Validation')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(metrics['train_accs'], label='Train')\n",
        "    plt.plot(metrics['val_accs'], label='Validation')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('pytorch_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Final accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Training time: {metrics['training_time']:.2f} seconds\")\n",
        "\n",
        "    # Print hardware utilization info\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"\\nGPU Memory Usage:\")\n",
        "        print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
        "        print(f\"Cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")"
      ]
    }
  ]
}